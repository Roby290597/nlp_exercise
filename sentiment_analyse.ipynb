{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roby290597/nlp_exercise/blob/main/bundestags_reden_analyse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.git', '21032.xml', 'bundestags_reden_analyse.ipynb', 'bundestag_aktuell.txt', 'bundestag_aktuell.xml', 'sentiment_analyse.ipynb']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AcL9PYxNY8zK"
      },
      "outputs": [],
      "source": [
        "# import xml.etree.ElementTree as ET\n",
        "# tree = ET.parse('bundestag_21019.xml')\n",
        "# root = tree.getroot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.bundestag.de/resource/blob/1115000/21032.xml\" ### Bundestag Reden XML Datei vom November 2025\n",
        "response = requests.get(url)\n",
        "\n",
        "with open(\"21032.xml\", \"wb\") as f:\n",
        "    f.write(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['vertrieb', 'herstellung', 'sitzung-ort', 'herausgeber', 'issn', 'wahlperiode', 'sitzung-nr', 'sitzung-datum', 'sitzung-start-uhrzeit', 'sitzung-ende-uhrzeit', 'sitzung-naechste-datum', 'start-seitennr']\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "url = \"https://www.bundestag.de/resource/blob/1115000/21032.xml\" ### Bundestag Reden XML Datei vom November 2025\n",
        "response = requests.get(url)\n",
        "\n",
        "# XML in einen Tree parsen\n",
        "tree = ET.ElementTree(ET.fromstring(response.content))\n",
        "\n",
        "# Wurzel-Element abrufen\n",
        "root = tree.getroot()\n",
        "\n",
        "print(root.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlzCruHSZDml",
        "outputId": "53a5275d-598d-443c-f80c-f7bf7a3c51d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'vertrieb': 'Bundesanzeiger Verlag GmbH, Postfach 1 0 05 34, 50445 Köln, Telefon (02 21) 97 66 83 40, Fax (02 21) 97 66 83 44, www.bundesanzeiger-verlag.de',\n",
              " 'herstellung': 'H. Heenemann GmbH  Co. KG, Buch- und Offsetdruckerei, Bessemerstraße 83–91, 12103 Berlin, www.heenemann-druck.de',\n",
              " 'sitzung-ort': 'Berlin',\n",
              " 'herausgeber': 'Deutscher Bundestag',\n",
              " 'issn': '0722-7980',\n",
              " 'wahlperiode': '21',\n",
              " 'sitzung-nr': '32',\n",
              " 'sitzung-datum': '10.10.2025',\n",
              " 'sitzung-start-uhrzeit': '09:00',\n",
              " 'sitzung-ende-uhrzeit': '15:13',\n",
              " 'sitzung-naechste-datum': '15.10.2025',\n",
              " 'start-seitennr': '3443'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root.attrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6CHLV8cZDp2",
        "outputId": "95be7714-6664-4d74-831b-8e8d7fe56382"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'anlagen-typ': 'Entschuldigte Abgeordnete'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root[2][1][1].attrib    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DOPPELT: Mandy Eißing\n",
            "DOPPELT: Peter Bohnhof\n",
            "DOPPELT: Mahmut Özdemir\n",
            "DOPPELT: Klaus Wiener\n",
            "DOPPELT: Mirze Edis\n"
          ]
        }
      ],
      "source": [
        "# Extrahiere Rednernamen und alle zugehörigen Redetexte aus dem XML, wie sie in <rede> gespeichert sind\n",
        "def extract_speeches(root):\n",
        "    speeches = []\n",
        "    for rede in root.findall(\".//rede\"):\n",
        "        # Rednername aus dem ersten <p klasse=\"redner\"> extrahieren\n",
        "        redner_p = rede.find(\"./p[@klasse='redner']\")\n",
        "        if redner_p is not None:\n",
        "            # Versuche Vorname und Nachname aus verschachtelten Tags zu holen\n",
        "            name_tag = redner_p.find(\".//name\")\n",
        "            if name_tag is not None:\n",
        "                vorname = name_tag.findtext(\"vorname\", default=\"\").strip()\n",
        "                nachname = name_tag.findtext(\"nachname\", default=\"\").strip()\n",
        "                redner_name = f\"{vorname} {nachname}\".strip()\n",
        "            else:\n",
        "                # Fallback: Text direkt aus <p> nehmen\n",
        "                redner_name = redner_p.text.split(\":\")[0].strip() if redner_p.text else \"\"\n",
        "        else:\n",
        "            redner_name = \"\"\n",
        "\n",
        "        # Alle <p> mit Redeinhalt (ohne klasse=\"redner\" und ohne <kommentar>)\n",
        "        rede_text = []\n",
        "        for p in rede.findall(\"./p\"):\n",
        "            if p.get(\"klasse\") != \"redner\" and p.text and p.text.strip():\n",
        "                rede_text.append(p.text.strip())\n",
        "        full_text = \"\\n\".join(rede_text)\n",
        "        speeches.append({\"name\": redner_name, \"text\": full_text})\n",
        "    return speeches\n",
        "\n",
        "\n",
        "reden = {}\n",
        "redner = []\n",
        "# Beispiel-Ausgabe\n",
        "for speech in extract_speeches(root):\n",
        "    #print(f\"Redner: {speech['name']}\\nRede: {speech['text'][:200]}...\\n\")\n",
        "    if speech['name'] not in redner:\n",
        "        redner.append(speech['name'])\n",
        "        reden[speech['name']] = speech['text']\n",
        "    else:\n",
        "        print(\"DOPPELT:\", speech['name'])\n",
        "        reden[speech['name']] += \"\\n Nächste Rede:\" + speech['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Frau Präsidentin! Meine Damen und Herren! Liebe Bürgerinnen und Bürger! Stellen Sie sich vor, Sie schuften jeden Tag, geben alles für Ihren Job, für Ihre Familie, für unser Land, und dann kommt diese Regierung und verspricht Ihnen Schutz, Fairness und mehr Wohlstand. Aber was passiert wirklich? Für diese Regierung gilt nämlich in der ganzen Legislatur der Grundsatz: Erst versprochen, dann gebrochen.\\nUnd genau das sehen wir auch bei diesem sogenannten Tariftreuegesetz: ein Gesetz\\xa0– ein Wolf im Schafspelz geradezu\\xa0–, das allen die Freiheit raubt, ob Arbeitnehmern, Unternehmern oder einfach nur jemandem, der auf faire Chancen hofft. Dieses Thema betrifft uns alle.\\nSchauen wir uns deshalb den Gesetzentwurf mal etwas näher an. Die Regierung will bei öffentlichen Aufträgen nur noch Firmen berücksichtigen, die ein sogenanntes Tariftreueversprechen abgeben. Auf den ersten Blick klingt das nach Gerechtigkeit. Die Wahrheit ist eine andere: Es ist ein massiver Eingriff in unsere Wirtschaftsfreiheit, meine Damen und Herren.\\nKleine Betriebe, Familienunternehmen, der gesamte Mittelstand: All jene, die ohne die großen Gewerkschaften arbeiten, werden einfach kaltgestellt auf diese Art und Weise. Sie können nicht mehr konkurrieren, weil der Staat diktiert. Nur wer sich den Regeln der Regierung unterwirft, bekommt eine Chance.\\nDas zerstört den fairen Wettbewerb, treibt Preise hoch und belastet am Ende alle mit höheren Steuern und steigenden Preisen. Diese Regierung spricht von Entlastungen, aber sie schafft nur neue Ketten. Das ist das Markenzeichen dieser Regierung.\\nUnd dann natürlich die Bürokratie, die explodiert: neue Nachweise, Kontrollbehörden wie zum Beispiel eine Clearingstelle und eine Prüfstelle Bundestariftreue. Warum nicht gleich „Bundestreue“, meine Damen und Herren?\\n– Deutlich mehr als Sie.\\xa0– Alles wird überwacht und kontrolliert. Mit sozialer Marktwirtschaft hat das nichts zu tun, stattdessen: mehr Beamte, höhere Kosten, totales Chaos. Zig Millionen Euro fließen in diesen Unsinn statt in echte Jobs und höhere Löhne.\\nDie Regierung redet von Sozialpartnerschaft, aber sie zertrampelt alles. Die Tarifautonomie, das Recht, frei zu verhandeln zwischen Arbeitgebern und Arbeitnehmern, wird ignoriert. Dieses Gesetz ist eben keine Stärkung der Tarifautonomie.\\nStattdessen befiehlt der Staat per Verordnung, was gezahlt werden muss. Das ist kein Schutz; das ist Zwang.\\nDieses Gesetz ist aber auch handwerklich schlecht gemacht. Es gibt dort Regelungslücken über Regelungslücken. Ein ganz einfaches Beispiel: Was passiert mit einem Arbeitnehmer, der für die Firma, für die er tätig ist, an verschiedenen Objekten tätig ist\\xa0– bei einem öffentlichen und bei einem privaten? Tariftreue: Ja, nein, vielleicht? Das Gesetz gibt keine Auskunft.\\nViele weitere unklare Regelungen und gleichzeitig Vertragsstrafen für die Unternehmen: So funktionieren real existierender Sozialismus und Planwirtschaft, aber keine Marktwirtschaft.\\nUnd wer leidet darunter? Alle Arbeitnehmer, die dann weniger flexibel sind, und die Unternehmer, die ihre Freiheit verlieren. Die Regierung lügt, wenn sie behauptet, das hilft ihnen. Es knebelt sie.\\nAber es geht auch noch tiefer. Rechtlich ist das ein Pulverfass. Das Vergaberecht könnte durch das Gesetz unterlaufen werden, ebenso EU-Regeln zur Dienstleistungsfreiheit. Die Regierung weiß das. Aber sie drückt es durch, weil es um Ideologie geht, nicht um Vernunft.\\nMehr Staat, weniger Freiheit. Sie zwingen Unternehmen in Tarifverträge, die niemand braucht, und rauben allen die Selbstbestimmung. Das ist nicht fair; das ist unredlich.\\nLeere Versprechen, nichts als leere Versprechen\\xa0– wie bei so vielen anderen Themen: stets groß angekündigt und dann zusammengebrochen.\\nDieses Tariftreuegesetz ist der Beweis: Diese Regierung bremst unser Land, bremst die Fleißigen. Sie bauen einen Staat auf, der uns alle ärmer macht.\\nWir von der AfD kämpfen für wahre Freiheit, für einen Markt ohne Zwang,\\nfür weniger Bürokratie, für gleiche Chancen für alle. Wir wollen Tarifverträge, die freiwillig entstehen, nicht erzwungen sind, also Tarifautonomie. Und wir fordern: Steuern senken und unsere Jobs schützen vor unfairer Konkurrenz.\\nDeshalb appelliere ich an alle: Lassen Sie sich nicht länger täuschen! Stehen Sie auf gegen diese unredliche Politik!\\nNur die AfD schützt die Arbeit und kann Deutschland wieder starkmachen.\\nFür die Fraktion der CDU/CSU hat nun Frau Abgeordnete Sandra Carstensen das Wort.\\n Nächste Rede:Herr Präsident! Meine Damen und Herren! Liebe Bürger! Im Herzen des Ruhrgebiets, aus dem ich komme, schließt die letzte große Fabrik ihre Tore. Tausende hart arbeitende Familienväter und -mütter stehen plötzlich auf der Straße\\xa0– ohne Job, ohne Zukunft. Die einst blühende Region verfällt vollkommen: Betriebe leer, Geschäfte pleite. Das ist leider keine Science-Fiction. Das ist bittere Realität, die uns die sogenannte sozialökologische Transformation der Industrie bereitet, wenn wir diesen Wahnsinn nicht stoppen.\\nDie SED-Nachfolgepartei Die Linke\\nfordert eine totale Umwälzung unserer Schlüsselindustrien Chemie, Stahl und Automobil. Staatliche Millionenförderungen aus Steuergeldern sollen fließen, wenn strenge staatliche Vorgaben erfüllt sind. Mehr Macht für linke Gewerkschaften, Tarifzwang statt Freiheit, noch höhere Steuern und noch mehr sozialistische Umverteilung:\\nDas toppt sogar die chaotische Politik der Bundesregierung und ist genauso schlimm wie die Deindustrialisierungspolitik der Grünen.\\nDie Ungewissheit zur Zukunft der Verbrennungsmotoren ganz aktuell und das mögliche Verbot lähmen die Wirtschaft. Die Industrie, die Menschen warten auf Antworten. Hunderttausende Jobs sind in Gefahr. Unter dem Strich gehört alles in die gefährliche Kategorie „linke Planwirtschaft“.\\nDie Ideologieproduktionsstätte der Linken, die Rosa-Luxemburg-Stiftung, fordert ganz öffentlich eine gesellschaftliche Revolution,\\nganz im Geiste des alten Genossen Lenin. Obwohl wir heute aus bitterer Erfahrung längst wissen und gelernt haben: Jede sozialistische Umwälzung vertieft Ungleichheiten, statt sie zu beseitigen, und zerstört jede freiheitliche Wirtschaft von innen heraus.\\nLassen Sie uns die Fakten betrachten. Eine solche Transformation ist eine direkte Bedrohung für Jobs und Wohlstand. Die AfD stellt sich daher klar und deutlich gegen jede sozialökologische Umwälzung.\\nUnd schauen wir noch auf die Energiekrise. Grüne Politik hat bereits die Strompreise in die Höhe getrieben, indem sie auf unzuverlässige Wind- und Solarenergie setzt statt auf bewährte Energiequellen wie Kohle, Gas und Kernkraft.\\nEnergieintensive Branchen wie Stahl und Chemie wandern ab nach Asien, nach Amerika, wo es keine solch irren staatlichen Eingriffe gibt, meine Damen und Herren.\\nStatt Innovation zu fördern, diktiert der Staat, was gebaut werden darf. Das Ergebnis: immer mehr Arbeitslosigkeit in Regionen, wo Tausende gut bezahlter Jobs an diesen Branchen hängen. Und wer zahlt die Zeche? Der kleine Mann! Und Sie verdrehen das. Das bedeutet höhere Steuern für die Umverteilung, teurere Heizungen durch grüne Auflagen, steigende Mieten und weniger soziale Sicherheit. Die Linken reden von Gerechtigkeit; aber ihre Pläne treffen die Ärmsten am härtesten.\\nWir als AfD-Fraktion sehen das ganz klar: Diese Politik ist Planwirtschaft, die unsere Freiheit einschränkt und den Klimawandel als Vorwand nutzt.\\nDabei wird bewusst verdrängt: Tausende Wissenschaftler bezweifeln den menschengemachten Anteil am Klimawandel. Historische Warmphasen gab es immer\\xa0– ohne Katastrophen.\\nWarum soll sich das ändern? Sagen Sie es! Warum also Milliarden in deutsche Grüntechnologien pumpen statt in echte Infrastruktur, Bildung oder echten Umweltschutz, wie zum Beispiel saubere Flüsse?\\nDer EU-Green-Deal, den diese Transformation unterstützt, macht Deutschland zur Hochpreisinsel. Wir verlieren Souveränität an Brüssel, subventionieren unrentable Projekte und schwächen unsere Industrie.\\nUnsere Vorschläge gegen die Industrialisierung werden noch ignoriert, obwohl sie Jobs schützen würden. Aber sie sind richtig und notwendig, um unsere Wirtschaft und damit auch unseren Sozialstaat und unsere Freiheit zu retten. Wir fordern darum den sofortigen Ausstieg aus dem Pariser Klimaabkommen, die Abschaffung der CO2-Steuer und die Rückkehr zur Kernenergie.\\nFür günstige stabile Energie, Deregulierung statt Verbote: Lassen Sie den Markt und Innovation walten! Weniger Bürokratie für Unternehmen, niedrigere Steuern, der Schutz nationaler Interessen, die Stärkung der Industrie durch Technologieoffenheit, nicht durch Gewerkschaftsmacht und staatliche Diktate:\\nNur so sichern wir Arbeitsplätze, Wettbewerbsfähigkeit und Freiheit für die Deutschen.\\nIch bin gleich fertig, Herr Präsident.\\xa0– Eine sozialökologische Transformation ist kein Weg in die Zukunft, sondern in den Ruin. Stoppen Sie diesen Irrsinn! Kämpfen wir für eine starke, freie Wirtschaft und ein blühendes Deutschland.\\nVielen Dank.\\nVielen Dank.\\xa0– Der nächste Redner ist Mahmut Özdemir von der SPD-Fraktion.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reden['Peter Bohnhof']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'vorname': 'Bärbel', 'nachname': 'Bas', 'fraktion': ''}\n",
            "{'vorname': 'Hans-Jürgen', 'nachname': 'Goßner', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Wilfried', 'nachname': 'Oellers', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Ricarda', 'nachname': 'Lang', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Pascal', 'nachname': 'Meiser', 'fraktion': 'Die Linke'}\n",
            "{'vorname': 'Dagmar', 'nachname': 'Schmidt', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Peter', 'nachname': 'Bohnhof', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Sandra', 'nachname': 'Carstensen', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Armin', 'nachname': 'Grau', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Nora', 'nachname': 'Seitz', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Leif-Erik', 'nachname': 'Holm', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Peter', 'nachname': 'Aumer', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Jan', 'nachname': 'Dieren', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Elisabeth', 'nachname': 'Winkelmeier-Becker', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Tobias Matthias', 'nachname': 'Peterka', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Axel', 'nachname': 'Müller', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Lena', 'nachname': 'Gumnior', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Mahmut', 'nachname': 'Özdemir', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Luke', 'nachname': 'Hoß', 'fraktion': 'Die Linke'}\n",
            "{'vorname': 'Christian', 'nachname': 'Moser', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Rainer', 'nachname': 'Galla', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Maja', 'nachname': 'Wallstein', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Helge', 'nachname': 'Limburg', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Johannes', 'nachname': 'Wiegelmann', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Knuth', 'nachname': 'Meyer-Soltau', 'fraktion': 'AfD'}\n",
            "{'vorname': 'David', 'nachname': 'Preisendanz', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Wolfram', 'nachname': 'Weimer', 'fraktion': ''}\n",
            "{'vorname': 'Götz', 'nachname': 'Frömming', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Elisabeth', 'nachname': 'Kaiser', 'fraktion': ''}\n",
            "{'vorname': 'Katrin', 'nachname': 'Göring-Eckardt', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Mandy', 'nachname': 'Eißing', 'fraktion': 'Die Linke'}\n",
            "{'vorname': 'Ottilie', 'nachname': 'Klein', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Sven', 'nachname': 'Wendorf', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Franziska', 'nachname': 'Kersten', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Sepp', 'nachname': 'Müller', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Wolfgang', 'nachname': 'Dahler', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Birgit', 'nachname': 'Bessin', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Felix', 'nachname': 'Döring', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Misbah', 'nachname': 'Khan', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Konrad', 'nachname': 'Körner', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Anna', 'nachname': 'Rathert', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Reem', 'nachname': 'Alabali Radovan', 'fraktion': ''}\n",
            "{'vorname': 'Rocco', 'nachname': 'Kever', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Nicolas', 'nachname': 'Zippelius', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Claudia', 'nachname': 'Roth', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Mirze', 'nachname': 'Edis', 'fraktion': 'Die Linke'}\n",
            "{'vorname': 'Klaus', 'nachname': 'Wiener', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Johann', 'nachname': 'Martel', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Thomas', 'nachname': 'Bareiß', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Matthias', 'nachname': 'Gastel', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Michael', 'nachname': 'Donth', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Ulrich', 'nachname': 'von Zons', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Anja', 'nachname': 'Troff-Schaffarzyk', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Luigi', 'nachname': 'Pantisano', 'fraktion': 'Die Linke'}\n",
            "{'vorname': 'Henning', 'nachname': 'Rehbaum', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Maximilian', 'nachname': 'Kneller', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Jakob', 'nachname': 'Blankenburg', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Günter', 'nachname': 'Baumgartner', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Katherina', 'nachname': 'Reiche', 'fraktion': ''}\n",
            "{'vorname': 'Steffen', 'nachname': 'Kotré', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Daniel', 'nachname': 'Walter', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Katrin', 'nachname': 'Uhlig', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Jörg', 'nachname': 'Cezanne', 'fraktion': 'Die Linke'}\n",
            "{'vorname': 'Tilman', 'nachname': 'Kuban', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Raimond', 'nachname': 'Scheirich', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Agnes', 'nachname': 'Conrad', 'fraktion': 'Die Linke'}\n",
            "{'vorname': 'Andreas', 'nachname': 'Lenz', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Andreas', 'nachname': 'Audretsch', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Dirk', 'nachname': 'Brandes', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Bernd', 'nachname': 'Rützel', 'fraktion': 'SPD'}\n",
            "{'vorname': 'Michael', 'nachname': 'Kellner', 'fraktion': 'BÜNDNIS\\xa090/DIE GRÜNEN'}\n",
            "{'vorname': 'Lars', 'nachname': 'Rohwer', 'fraktion': 'CDU/CSU'}\n",
            "{'vorname': 'Gerrit', 'nachname': 'Huy', 'fraktion': 'AfD'}\n",
            "{'vorname': 'Vanessa', 'nachname': 'Zobel', 'fraktion': 'CDU/CSU'}\n"
          ]
        }
      ],
      "source": [
        "# Extrahiere alle Redner aus der <rednerliste> im XML\n",
        "def extract_all_speakers(root):\n",
        "    speakers = []\n",
        "    for rednerliste in root.findall(\".//rednerliste\"):\n",
        "        for redner in rednerliste.findall(\"redner\"):\n",
        "            name_tag = redner.find(\"name\")\n",
        "            if name_tag is not None:\n",
        "                vorname = name_tag.findtext(\"vorname\", default=\"\").strip()\n",
        "                nachname = name_tag.findtext(\"nachname\", default=\"\").strip()\n",
        "                fraktion = name_tag.findtext(\"fraktion\", default=\"\").strip()\n",
        "                rolle_lang = name_tag.findtext(\"rolle/rolle_lang\", default=\"\").strip()\n",
        "                rolle_kurz = name_tag.findtext(\"rolle/rolle_kurz\", default=\"\").strip()\n",
        "                speakers.append({\n",
        "                    #\"id\": redner.get(\"id\"),\n",
        "                    \"vorname\": vorname,\n",
        "                    \"nachname\": nachname,\n",
        "                    \"fraktion\": fraktion,\n",
        "                    #\"rolle_lang\": rolle_lang,\n",
        "                    #\"rolle_kurz\": rolle_kurz\n",
        "                })\n",
        "                \n",
        "    return speakers\n",
        "\n",
        "redner_party = []\n",
        "all_speakers = extract_all_speakers(root)\n",
        "for speaker in all_speakers:\n",
        "    print(speaker)\n",
        "    redner_party.append((speaker['vorname'] + \" \" + speaker['nachname'], speaker['fraktion']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'Bärbel Bas': 1, 'Hans-Jürgen Goßner': 1, 'Wilfried Oellers': 1, 'Ricarda Lang': 1, 'Pascal Meiser': 1, 'Dagmar Schmidt': 1, 'Peter Bohnhof': 1, 'Sandra Carstensen': 1, 'Armin Grau': 1, 'Nora Seitz': 1, 'Leif-Erik Holm': 1, 'Peter Aumer': 1, 'Jan Dieren': 1, 'Elisabeth Winkelmeier-Becker': 1, 'Tobias Matthias Peterka': 1, 'Axel Müller': 1, 'Lena Gumnior': 1, 'Mahmut Özdemir': 1, 'Luke Hoß': 1, 'Christian Moser': 1, 'Rainer Galla': 1, 'Maja Wallstein': 1, 'Helge Limburg': 1, 'Johannes Wiegelmann': 1, 'Knuth Meyer-Soltau': 1, 'David Preisendanz': 1, 'Wolfram Weimer': 1, 'Götz Frömming': 1, 'Elisabeth Kaiser': 1, 'Katrin Göring-Eckardt': 1, 'Mandy Eißing': 1, 'Ottilie Klein': 1, 'Sven Wendorf': 1, 'Franziska Kersten': 1, 'Sepp Müller': 1, 'Wolfgang Dahler': 1, 'Birgit Bessin': 1, 'Felix Döring': 1, 'Misbah Khan': 1, 'Konrad Körner': 1, 'Anna Rathert': 1, 'Reem Alabali Radovan': 1, 'Rocco Kever': 1, 'Nicolas Zippelius': 1, 'Claudia Roth': 1, 'Mirze Edis': 1, 'Klaus Wiener': 1, 'Johann Martel': 1, 'Thomas Bareiß': 1, 'Matthias Gastel': 1, 'Michael Donth': 1, 'Ulrich von Zons': 1, 'Anja Troff-Schaffarzyk': 1, 'Luigi Pantisano': 1, 'Henning Rehbaum': 1, 'Maximilian Kneller': 1, 'Jakob Blankenburg': 1, 'Günter Baumgartner': 1, 'Katherina Reiche': 1, 'Steffen Kotré': 1, 'Daniel Walter': 1, 'Katrin Uhlig': 1, 'Jörg Cezanne': 1, 'Tilman Kuban': 1, 'Raimond Scheirich': 1, 'Agnes Conrad': 1, 'Andreas Lenz': 1, 'Andreas Audretsch': 1, 'Dirk Brandes': 1, 'Bernd Rützel': 1, 'Michael Kellner': 1, 'Lars Rohwer': 1, 'Gerrit Huy': 1, 'Vanessa Zobel': 1})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "party_list = [name for name, party in redner_party] \n",
        "\n",
        "counter = Counter(party_list)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vektor-Länge: 309\n",
            "\n",
            "Top-Terme (mit TF-IDF-Werten):\n",
            "und: 0.3627\n",
            "auch: 0.3174\n",
            "wir: 0.2947\n",
            "die: 0.2720\n",
            "der: 0.2267\n",
            "das: 0.2040\n",
            "ein: 0.1814\n",
            "für: 0.1587\n",
            "um: 0.1587\n",
            "es: 0.1360\n",
            "hat: 0.1133\n",
            "sind: 0.1133\n",
            "ist: 0.1133\n",
            "dass: 0.0907\n",
            "wer: 0.0907\n",
            "den: 0.0907\n",
            "haben: 0.0907\n",
            "kollegen: 0.0907\n",
            "unternehmen: 0.0907\n",
            "liebe: 0.0907\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# TF-IDF-Vektor für die Rede von Bärbel Bas erzeugen\n",
        "\n",
        "# Rede aus bestehendem Dictionary verwenden\n",
        "text = reden.get(\"Bärbel Bas\", \"\")\n",
        "if not text:\n",
        "    raise ValueError(\"Keine Rede für 'Bärbel Bas' im Dictionary 'reden' gefunden.\")\n",
        "\n",
        "# Deutsches Stopword-Set nutzen, TF-IDF berechnen\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform([text])  # Ergebnis als sparse matrix\n",
        "baerbel_vec = X.toarray()[0]          # dichter Vektor\n",
        "\n",
        "print(\"Vektor-Länge:\", baerbel_vec.shape[0])\n",
        "\n",
        "# Wichtigste Terme anzeigen (Top 20)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "top_n = 20\n",
        "top_idx = np.argsort(baerbel_vec)[::-1][:top_n]\n",
        "print(\"\\nTop-Terme (mit TF-IDF-Werten):\")\n",
        "for i in top_idx:\n",
        "    print(f\"{feature_names[i]}: {baerbel_vec[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\tiktoken\\load.py:168\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     token, rank = line.split()\n\u001b[32m    169\u001b[39m     ret[base64.b64decode(token)] = \u001b[38;5;28mint\u001b[39m(rank)\n",
            "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 1)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1783\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1779\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1677\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1676\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1677\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1678\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1679\u001b[39m         [\n\u001b[32m   1680\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1681\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1682\u001b[39m         ]\n\u001b[32m   1683\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1670\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1669\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1670\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1671\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1646\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1643\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1644\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m bpe_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1647\u001b[39m byte_encoder = bytes_to_unicode()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\tiktoken\\load.py:171\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError parsing line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken_bpe_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
            "\u001b[31mValueError\u001b[39m: Error parsing line b'\\x0e' in C:\\Users\\brand\\.cache\\huggingface\\hub\\models--mrm8488--t5-base-finetuned-emotion\\snapshots\\e44a316825f11230724b36412fbf1899c76e82de\\spiece.model",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelWithLMHead\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmrm8488/t5-base-finetuned-emotion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m model = AutoModelWithLMHead.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mmrm8488/t5-base-finetuned-emotion\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_emotion\u001b[39m(text):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1159\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2097\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2094\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2095\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2097\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2100\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2343\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2341\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2342\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2343\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2345\u001b[39m     logger.info(\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2348\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:119\u001b[39m, in \u001b[36mT5TokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m     logger.warning_once(\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    117\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mfrom_slow\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m._extra_ids = extra_ids\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\anaconda3\\envs\\job\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1785\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1781\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1782\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1783\u001b[39m     ).converted()\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1785\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1786\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1787\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1788\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1789\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
        "\n",
        "def get_emotion(text):\n",
        "  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
        "\n",
        "  output = model.generate(input_ids=input_ids,\n",
        "               max_length=2)\n",
        "  \n",
        "  dec = [tokenizer.decode(ids) for ids in output]\n",
        "  label = dec[0]\n",
        "  return label\n",
        "  \n",
        "get_emotion(\"i feel as if i havent blogged in ages are at least truly blogged i am doing an update cute\") # Output: 'joy'\n",
        " \n",
        "get_emotion(\"i have a feeling i kinda lost my best friend\") # Output: 'sadness'\n",
        "emotion_pipeline = pipeline(\"text-classification\", model=\"mrm8488/t5-base-finetuned-emotion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity: 0.23459944128990173\n"
          ]
        }
      ],
      "source": [
        "# pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # klein & schnell; gute Qualität\n",
        "\n",
        "def embed(texts):\n",
        "    # texts: str or list[str]\n",
        "    return model.encode(texts, convert_to_numpy=True, normalize_embeddings=False)\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    # numerisch stabil: L2-normalisieren dann dot\n",
        "    a = a / np.linalg.norm(a)\n",
        "    b = b / np.linalg.norm(b)\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "# Beispiel\n",
        "t1 = \"Das ist ein Beispieltext.\"\n",
        "t2 = \"Das ist ein Beispieltext.\"\n",
        "t2 = \"Döner\"\n",
        "t1 = \"Pizza\"\n",
        "v1, v2 = embed([t1, t2])\n",
        "print(\"Cosine similarity:\", cosine_sim(v1, v2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP50MDuGDSS8xYQVPOiGNTy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "job",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
